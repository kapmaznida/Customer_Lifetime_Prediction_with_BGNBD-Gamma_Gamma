# -*- coding: utf-8 -*-
"""customer_lifetime_value_prediction_bgnbd_gg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pu_412iMMUzWZddHV-5l0YguVEV5FN9i


## Mission 1
6 months CLTV Prediction
* Make a 6-month CLTV prediction for 2010-2011 UK customers.
* Interpret and evaluate the results you have obtained.
## **Attention!!**
* We expect cltv prediction to be made, not 6-month expected sales. In other words, continue by installing the BGNBD & GAMMA models directly and enter 6 in the month section for cltv prediction.
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

from sqlalchemy import create_engine
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import datetime as dt
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory


# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
pd.set_option('display.float_format', lambda x: '%.4f' % x)


#!pip install lifetimes

from lifetimes import BetaGeoFitter
from lifetimes import GammaGammaFitter

#pip install openpyxl

"""# Veritabanı"""

creds = {
    "user" : "group_02",
    "passwd" : "hayatguzelkodlarucuyor",
    "host" : "34.88.156.118",
    "port" : 3306,
    "db" : "group_02"
}

connstr = 'mysql+mysqlconnector://{user}:{passwd}@{host}:{port}/{db}'

conn = create_engine(connstr.format(**creds))

pd.read_sql_query("show databases",conn)

pd.read_sql_query("show tables",conn)

"""Veri Okuma"""

from pathlib import Path
DATA_DIR = Path("/home/nida/PycharmProjects/DSMLBC/Hws/week3/online_retail_II.xlsx")
#Çalışma için Year2010-2011 sheeti kullanılacak!
dataset_ = pd.read_excel(DATA_DIR, sheet_name="Year 2010-2011")

dataset = dataset_.copy()

dataset.head()

dataset.describe().T

dataset.columns

dataset_UK = pd.DataFrame(dataset[dataset["Country"] == "United Kingdom"])

dataset_UK.head()

dataset_UK.dropna(inplace = True)

dataset_UK = dataset[~dataset["Invoice"].str.contains("C", na=False)]

dataset_UK.head()

dataset_UK.describe().T

dataset_UK = dataset_UK[dataset_UK["Quantity"]>0]

dataset_UK = dataset_UK[dataset_UK["Price"]>0]

dataset_UK.describe().T

def outlier_thresholds(dataframe, variable):
    quartile1 = dataframe[variable].quantile(0.01)
    quartile3 = dataframe[variable].quantile(0.99)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

replace_with_thresholds(dataset_UK, "Quantity")

replace_with_thresholds(dataset_UK, "Price")

dataset_UK.describe().T

dataset_UK["TotalPrice"] = dataset_UK["Quantity"] * dataset_UK["Price"]

dataset_UK["InvoiceDate"].max()

today = dt.datetime(2011, 12, 11)

"""# Lifetime Data structure
* **recency**: Time since last purchase. Weekly. (according to analysis day before, here is user specific)
* **T**: How long before the analysis date was the first purchase made. Weekly.
* **frequency**: total number of repeat purchases (frequency>1)
* **monetary_value**: average earnings per purchase
"""

cltv_dataset_UK = dataset_UK.groupby("Customer ID").agg({'InvoiceDate': [lambda date: (date.max() - date.min()).days,
                                                                         lambda date: (today - date.min()).days],
                                                         'Invoice': lambda num: num.nunique(),
                                                         'TotalPrice': lambda TotalPrice: TotalPrice.sum()})

cltv_dataset_UK.head()

cltv_dataset_UK.columns = cltv_dataset_UK.columns.droplevel(0)

cltv_dataset_UK.head()

cltv_dataset_UK.columns = ['recency', 'T', 'frequency', 'monetary']

cltv_dataset_UK.head()

"""Expressing the monetary value of the average earnings per purchase"""

cltv_dataset_UK['monetary'] = cltv_dataset_UK["monetary"] / cltv_dataset_UK["frequency"]

"""We look at the customer values of those who have shopped more than once, that is, at least 2 times."""

cltv_dataset_UK = cltv_dataset_UK[(cltv_dataset_UK['frequency'] > 1)]

cltv_dataset_UK = cltv_dataset_UK[cltv_dataset_UK["monetary"] > 0]

"""* Expression of recency and T for BGNBD in weekly terms"""

cltv_dataset_UK["recency"] = cltv_dataset_UK["recency"] / 7

cltv_dataset_UK["T"] = cltv_dataset_UK["T"] / 7

"""# Establishment of BG-NBD Model

We put a penalty parameter to prevent overfitting.
"""

bgf = BetaGeoFitter(penalizer_coef=0.001)

bgf.fit(cltv_dataset_UK['frequency'], cltv_dataset_UK['recency'], cltv_dataset_UK['T'])

cltv_dataset_UK["expected_purc_1_week"] = bgf.predict(1,
                                               cltv_dataset_UK['frequency'],
                                               cltv_dataset_UK['recency'],
                                               cltv_dataset_UK['T'])

cltv_dataset_UK["expected_purc_1_month"] = bgf.predict(4,
                                               cltv_dataset_UK['frequency'],
                                               cltv_dataset_UK['recency'],
                                               cltv_dataset_UK['T'])

cltv_dataset_UK.head()

"""# Establishing the Gamma Gamma Model
expected average profit
"""

ggf = GammaGammaFitter(penalizer_coef=0.01)

ggf.fit(cltv_dataset_UK['frequency'], cltv_dataset_UK['monetary'])

ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                        cltv_dataset_UK['monetary']).head(10)

ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                        cltv_dataset_UK['monetary']).sort_values(ascending=False).head(10)

cltv_dataset_UK["expected_average_profit"] = ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                                                             cltv_dataset_UK['monetary'])

cltv_dataset_UK.sort_values("expected_average_profit", ascending=False).head(20)

cltv_dataset_UK.head()

"""# Calculation of CLTV with BG-NBD and GG model"""

cltv = ggf.customer_lifetime_value(bgf,
                                   cltv_dataset_UK['frequency'],
                                   cltv_dataset_UK['recency'],
                                   cltv_dataset_UK['T'],
                                   cltv_dataset_UK['monetary'],
                                   time=6,  # 6 aylık
                                   freq="W",  # T'nin frekans bilgisi.
                                   discount_rate=0.01)

cltv.head()

cltv.shape

cltv = cltv.reset_index()

cltv.head()

cltv.sort_values(by="clv", ascending=False).head(50)

cltv_final = cltv_dataset_UK.merge(cltv, on="Customer ID", how="left")

cltv_final.sort_values(by="clv", ascending=False).head()

cltv_final.sort_values(by="clv", ascending=False)[10:30]

"""Standardization"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(1, 50))

scaler.fit(cltv_final[["clv"]])

cltv_final["SCALED_CLTV"] = scaler.transform(cltv_final[["clv"]])

cltv_final.sort_values(by="clv", ascending=False)[10:30]

cltv_final.head()

"""#  Mission 2
## CLTV analysis consisting of different time periods
* Calculate 1-month and 12-month CLTV for 2010-2011 UK customers.
* Analyze the top 10 people at 1-month CLTV and the top 10 people at 12 months.
* Is there a difference? If so why could it be?
"""

cltv1 = ggf.customer_lifetime_value(bgf,
                                    cltv_dataset_UK['frequency'],
                                    cltv_dataset_UK['recency'],
                                    cltv_dataset_UK['T'],
                                    cltv_dataset_UK['monetary'],
                                    time=1,  # months
                                    freq="W",  # T haftalık
                                    discount_rate=0.01)

rfm_cltv1_final = cltv_dataset_UK.merge(cltv1, on="Customer ID", how="left")

rfm_cltv1_final.sort_values(by="clv", ascending=False).head(10)

cltv12 = ggf.customer_lifetime_value(bgf,
                                     cltv_dataset_UK['frequency'],
                                     cltv_dataset_UK['recency'],
                                     cltv_dataset_UK['T'],
                                     cltv_dataset_UK['monetary'],
                                     time=12,  # months
                                     freq="W",  # T haftalık
                                     discount_rate=0.01)

rfm_cltv12_final = cltv_dataset_UK.merge(cltv12, on="Customer ID", how="left")

rfm_cltv12_final.sort_values(by="clv", ascending=False).head(10)

"""When 1-month cltv and 12-month cltv are compared: The person who can be in the 10th place in 1 month has been replaced by someone else in 12 months. So in the longer term, the person with Customer ID 13694.0000 is more permanent."""
rfm_cltv12_final.head()

"""# Mission 3
## Segmentation and Action Recommendations

* For 2010*2011 UK customers, divide all your customers into 4 Groups (segments) according to 6-month CLTV and add the group names to the dataset.
* Make short 6-month action suggestions to the management for 2 groups you will choose from among 4 groups.
"""

cltv_final["cltv_segment"] = pd.qcut(cltv_final["SCALED_CLTV"], 4, labels=["D", "C", "B", "A"])

cltv_final["cltv_segment"].value_counts()

cltv_final.head()

cltv_final.groupby("cltv_segment")[["expected_purc_1_month", "expected_average_profit", "clv", "SCALED_CLTV"]].agg(
    {"count", "mean", "sum"})

"""*  2. CLTV skorlarına göre müşterileri 4 gruba ayırmak mantıklı mıdır?"""

cltv_final.groupby("cltv_segment").agg({"mean"})

"""# Mission 4
* Send the final table, which will consist of the following variables, to the database.
* Create the name of the table as name_surname.
* The table name must be entered in the "name" section of the relevant function.
* Customer ID, recency, T, frequency, monetary, expected_purc_1_week, expected_purc_1_month, expected_average_profit, clv, scaled_clv, segment
"""

cltv_final = cltv_final.reset_index()

cltv_final["Customer ID"] = cltv_final["Customer ID"].astype(int)

cltv_final.head()

cltv_final.to_sql(name='Nida_Kapmaz', con=conn, if_exists='replace', index=False)

pd.read_sql_query("show tables",conn)
conn.close()

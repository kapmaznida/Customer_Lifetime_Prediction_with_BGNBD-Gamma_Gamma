# -*- coding: utf-8 -*-
"""customer_lifetime_value_prediction_bgnbd_gg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pu_412iMMUzWZddHV-5l0YguVEV5FN9i

# İş Problemi
Bir e-ticaret sitesi müşteri aksiyonları için müşterilerinin CLTV değerlerine göre ileriye dönük bir projeksiyon yapılmasını istemektedir.

Elinizdeki veriseti ile 1 aylık yada 6 aylık zaman periyotları içerisinde en çok gelir getirebilecek müşterileri tespit etmek mümkün müdür?

# Veri Seti Hikayesi
Online Retail II isimli veri seti İngiltere merkezli online bir satış
mağazasının 01/12/2009 - 09/12/2011 tarihleri arasındaki satışlarını
içeriyor.

Bu şirketin ürün kataloğunda hediyelik eşyalar yer alıyor. Promosyon
ürünleri olarak da düşünülebilir.

Çoğu müşterisinin toptancı olduğu bilgisi de mevcut.

# Değişkenler
* InvoiceNo – Fatura Numarası (Eğer bu kod C ile başlıyorsa işlemin iptal edildiğini ifade eder.)
* StockCode – Ürün kodu (Her bir ürün için eşsiz numara.)
* Description – Ürün ismi
* Quantity – Ürün adedi (Faturalardaki ürünlerden kaçar tane satıldığını ifade etmektedir.)
* InvoiceDate – Fatura tarihi
* UnitPrice – Fatura fiyatı (Sterlin)
* CustomerID – Eşsiz müşteri numarası
* Country – Ülke ismi

## Veri Setine Erişim & Aşamalar
* ▪ Veriyi excel dosyasından ya da Google Cloud sunucusundan edinebilirsiniz.
* ▪ Eğer sunucuda olan veriye erişmek isterseniz grubunuz için oluşturulmuş olan bağlantı bilgilerini kullanabilirsiniz.
* ▪ Bağlantı bilgilerini mentorunuzdan alabilirsiniz.

## Görev 1
6 aylık CLTV Prediction
* 2010-2011 UK müşterileri için 6 aylık CLTV prediction yapınız.
* Elde ettiğiniz sonuçları yorumlayıp üzerinde değerlendirme yapınız.
## **Dikkat!!**
* 6 aylık expected sales değil cltv prediction yapılmasını bekliyoruz. Yani direkt BGNBD & GAMMA modellerini kurarak devam ediniz ve cltv prediction için ay bölümüne 6 giriniz.
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

from sqlalchemy import create_engine
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import datetime as dt
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory


# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
pd.set_option('display.float_format', lambda x: '%.4f' % x)


#!pip install lifetimes

from lifetimes import BetaGeoFitter
from lifetimes import GammaGammaFitter

#pip install openpyxl

"""# Veritabanı"""

creds = {
    "user" : "group_02",
    "passwd" : "hayatguzelkodlarucuyor",
    "host" : "34.88.156.118",
    "port" : 3306,
    "db" : "group_02"
}

connstr = 'mysql+mysqlconnector://{user}:{passwd}@{host}:{port}/{db}'

conn = create_engine(connstr.format(**creds))

pd.read_sql_query("show databases",conn)

pd.read_sql_query("show tables",conn)

"""Veri Okuma"""

from pathlib import Path
DATA_DIR = Path("/home/nida/PycharmProjects/DSMLBC/Hws/week3/online_retail_II.xlsx")
#Çalışma için Year2010-2011 sheeti kullanılacak!
dataset_ = pd.read_excel(DATA_DIR, sheet_name="Year 2010-2011")

dataset = dataset_.copy()

dataset.head()

dataset.describe().T

dataset.columns

dataset_UK = pd.DataFrame(dataset[dataset["Country"] == "United Kingdom"])

dataset_UK.head()

dataset_UK.dropna(inplace = True)

dataset_UK = dataset[~dataset["Invoice"].str.contains("C", na=False)]

dataset_UK.head()

dataset_UK.describe().T

dataset_UK = dataset_UK[dataset_UK["Quantity"]>0]

dataset_UK = dataset_UK[dataset_UK["Price"]>0]

dataset_UK.describe().T

def outlier_thresholds(dataframe, variable):
    quartile1 = dataframe[variable].quantile(0.01)
    quartile3 = dataframe[variable].quantile(0.99)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

replace_with_thresholds(dataset_UK, "Quantity")

replace_with_thresholds(dataset_UK, "Price")

dataset_UK.describe().T

dataset_UK["TotalPrice"] = dataset_UK["Quantity"] * dataset_UK["Price"]

dataset_UK["InvoiceDate"].max()

today = dt.datetime(2011, 12, 11)

"""# Lifetime Veri yapısı

*  **recency**: Son satın alma üzerinden geçen zaman. Haftalık. (daha önce analiz gününe göre, burada kullanıcı özelinde)
*  **T**: Analiz tarihinden ne kadar süre önce ilk satın alma yapılmış. Haftalık.
*  **frequency**: tekrar eden toplam satın alma sayısı (frequency>1)
*  **monetary_value**: satın alma başına ortalama kazanç
"""

cltv_dataset_UK = dataset_UK.groupby("Customer ID").agg({'InvoiceDate': [lambda date: (date.max() - date.min()).days,
                                                                         lambda date: (today - date.min()).days],
                                                         'Invoice': lambda num: num.nunique(),
                                                         'TotalPrice': lambda TotalPrice: TotalPrice.sum()})

cltv_dataset_UK.head()

cltv_dataset_UK.columns = cltv_dataset_UK.columns.droplevel(0)

cltv_dataset_UK.head()

cltv_dataset_UK.columns = ['recency', 'T', 'frequency', 'monetary']

cltv_dataset_UK.head()

"""monetary değerinin satın alma başına ortalama kazanç ifade edilmesi"""

cltv_dataset_UK['monetary'] = cltv_dataset_UK["monetary"] / cltv_dataset_UK["frequency"]

"""1den fazla yani en az 2 kez alışveriş yapmış olanların müşteri değerlerine bakarız."""

cltv_dataset_UK = cltv_dataset_UK[(cltv_dataset_UK['frequency'] > 1)]

cltv_dataset_UK = cltv_dataset_UK[cltv_dataset_UK["monetary"] > 0]

"""* BGNBD için recency ve T'nin haftalık cinsten ifade edilmesi"""

cltv_dataset_UK["recency"] = cltv_dataset_UK["recency"] / 7

cltv_dataset_UK["T"] = cltv_dataset_UK["T"] / 7

"""# BG-NBD Modelinin Kurulması

overfitting önüne geçmek için ceza parametresi koyuyoruz.
"""

bgf = BetaGeoFitter(penalizer_coef=0.001)

bgf.fit(cltv_dataset_UK['frequency'], cltv_dataset_UK['recency'], cltv_dataset_UK['T'])

cltv_dataset_UK["expected_purc_1_week"] = bgf.predict(1,
                                               cltv_dataset_UK['frequency'],
                                               cltv_dataset_UK['recency'],
                                               cltv_dataset_UK['T'])

cltv_dataset_UK["expected_purc_1_month"] = bgf.predict(4,
                                               cltv_dataset_UK['frequency'],
                                               cltv_dataset_UK['recency'],
                                               cltv_dataset_UK['T'])

cltv_dataset_UK.head()

"""# Gamma Gamma Modelinin Kurulması
expected average profit
"""

ggf = GammaGammaFitter(penalizer_coef=0.01)

ggf.fit(cltv_dataset_UK['frequency'], cltv_dataset_UK['monetary'])

ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                        cltv_dataset_UK['monetary']).head(10)

ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                        cltv_dataset_UK['monetary']).sort_values(ascending=False).head(10)

cltv_dataset_UK["expected_average_profit"] = ggf.conditional_expected_average_profit(cltv_dataset_UK['frequency'],
                                                                             cltv_dataset_UK['monetary'])

cltv_dataset_UK.sort_values("expected_average_profit", ascending=False).head(20)

cltv_dataset_UK.head()

"""# BG-NBD ve GG modeli ile CLTV hesaplanması"""

cltv = ggf.customer_lifetime_value(bgf,
                                   cltv_dataset_UK['frequency'],
                                   cltv_dataset_UK['recency'],
                                   cltv_dataset_UK['T'],
                                   cltv_dataset_UK['monetary'],
                                   time=6,  # 6 aylık
                                   freq="W",  # T'nin frekans bilgisi.
                                   discount_rate=0.01)

cltv.head()

cltv.shape

cltv = cltv.reset_index()

cltv.head()

cltv.sort_values(by="clv", ascending=False).head(50)

cltv_final = cltv_dataset_UK.merge(cltv, on="Customer ID", how="left")

cltv_final.sort_values(by="clv", ascending=False).head()

cltv_final.sort_values(by="clv", ascending=False)[10:30]

"""Standardization"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(1, 50))

scaler.fit(cltv_final[["clv"]])

cltv_final["SCALED_CLTV"] = scaler.transform(cltv_final[["clv"]])

cltv_final.sort_values(by="clv", ascending=False)[10:30]

cltv_final.head()

"""#  Görev 2
## Farklı zaman periyotlarından oluşan CLTV analizi
* 2010-2011 UK müşterileri  için 1 aylık ve 12 aylık CLTV hesaplayınız.
* 1 aylık CLTV'de en yüksek olan 10 kişi ile 12 aylık'taki en yüksek 10 kişiyi analiz ediniz.
* Fark var mı? Varsa neden olabilir?
"""

cltv1 = ggf.customer_lifetime_value(bgf,
                                    cltv_dataset_UK['frequency'],
                                    cltv_dataset_UK['recency'],
                                    cltv_dataset_UK['T'],
                                    cltv_dataset_UK['monetary'],
                                    time=1,  # months
                                    freq="W",  # T haftalık
                                    discount_rate=0.01)

rfm_cltv1_final = cltv_dataset_UK.merge(cltv1, on="Customer ID", how="left")

rfm_cltv1_final.sort_values(by="clv", ascending=False).head(10)

cltv12 = ggf.customer_lifetime_value(bgf,
                                     cltv_dataset_UK['frequency'],
                                     cltv_dataset_UK['recency'],
                                     cltv_dataset_UK['T'],
                                     cltv_dataset_UK['monetary'],
                                     time=12,  # months
                                     freq="W",  # T haftalık
                                     discount_rate=0.01)

rfm_cltv12_final = cltv_dataset_UK.merge(cltv12, on="Customer ID", how="left")

rfm_cltv12_final.sort_values(by="clv", ascending=False).head(10)

"""1 aylık cltv ile 12 aylık cltv karşılaştırıldığında: 1 aylıkta 10. sırada olabilen kişinin yerine 12 aylıkta başkası geçmiş. Demekki daha uzun vadede Customer ID 13694.0000 olan kişi daha kalıcı."""

rfm_cltv12_final.head()

"""# Görev 3
## Segmentasyon ve Aksiyon Önerileri

* 2010*2011 UK müşterileri için 6 aylık CLTV'ye göre tüm müşterilerinizi 4 Gruba (segmente) ayırınız ve grup isimlerini verisetine ekleyiniz.
* 4 grup içerisinden seçeceğiniz 2 grup için yönetime kısa kısa 6 aylık aksiyon önerilerinde bulununuz.
"""

cltv_final["cltv_segment"] = pd.qcut(cltv_final["SCALED_CLTV"], 4, labels=["D", "C", "B", "A"])

cltv_final["cltv_segment"].value_counts()

cltv_final.head()

cltv_final.groupby("cltv_segment")[["expected_purc_1_month", "expected_average_profit", "clv", "SCALED_CLTV"]].agg(
    {"count", "mean", "sum"})

"""*  2. CLTV skorlarına göre müşterileri 4 gruba ayırmak mantıklı mıdır?"""

cltv_final.groupby("cltv_segment").agg({"mean"})

"""# Görev 4
*  Aşağıdaki değişkenlerden oluşacak final tablosunu veri tabanına gönderiniz.
*  tablonun adını isim_soyisim şeklinde oluşturunuz.
*  Tablo ismi ilgili fonksiyonda "name" bölümüne girilmelidir.
* *  Customer ID, recency, T, frequency, monetary, expected_purc_1_week, expected_purc_1_month, expected_average_profit, clv, scaled_clv, segment 
"""

cltv_final = cltv_final.reset_index()

cltv_final["Customer ID"] = cltv_final["Customer ID"].astype(int)

cltv_final.head()

cltv_final.to_sql(name='Nida_Kapmaz', con=conn, if_exists='replace', index=False)

pd.read_sql_query("show tables",conn)
conn.close()